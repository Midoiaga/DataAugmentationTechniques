{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Corpus EnlargementðŸ”\n","This notebook takes the parallel corpus between catalan and english that is stored in ./data and using different techniques create new files that will be used later to train different models.\\\n","The resulting dataset and the models trained with them are summarised in the spreadsheet \"Data & Models Summary\"."],"metadata":{"id":"D8dsdtSHkOVI"}},{"cell_type":"markdown","source":["## Previous steps\n","Imports, mounting drive, setting the working directory."],"metadata":{"id":"9NodOkVPyWiD"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece\n","! pip install --upgrade torch==2.0.0 --extra-index-url https://download.pytorch.org/whl/cu116\n","!pip install sacremoses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14U4t5fSLOiY","executionInfo":{"status":"ok","timestamp":1683193949236,"user_tz":-120,"elapsed":48216,"user":{"displayName":"Maite Heredia","userId":"12373846054036038986"}},"outputId":"4bdae0df-1c84-4b36-88ea-91faa0c5f1a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n","Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.12.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (16.0.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.25.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=cb8648df8c2af7b234ad5a1099aa18ff066d365af74424a9e9e9ba17f448e96e\n","  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n","Successfully built sacremoses\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.0.53\n"]}]},{"cell_type":"code","source":["from transformers import MarianTokenizer, MarianMTModel\n","from typing import List\n","import os\n","from os import path\n","import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","import random"],"metadata":{"id":"866cN8eBR_Ky","executionInfo":{"status":"ok","timestamp":1683195043453,"user_tz":-120,"elapsed":9346,"user":{"displayName":"Maite Heredia","userId":"12373846054036038986"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"63f763f3-4b3b-47b4-f150-044a7fff5739"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flJ7OPAlJplG","executionInfo":{"status":"ok","timestamp":1683465455684,"user_tz":-120,"elapsed":112675,"user":{"displayName":"Maite Heredia","userId":"12373846054036038986"}},"outputId":"bc9a1777-afe4-470f-93f0-3b03a05facc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["data_path = \"/content/drive/MyDrive/Final-Project-MT/data\""],"metadata":{"id":"wYsHzgGLXMbn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Backtranslation\n","For this technique, we will be using the translations systems available at https://huggingface.co/Helsinki-NLP."],"metadata":{"id":"a4-a-HSoMEoi"}},{"cell_type":"markdown","source":["First we select the models we are going to use."],"metadata":{"id":"vkNC-r7UzG6I"}},{"cell_type":"code","source":["target_source = \"Helsinki-NLP/opus-mt-en-ca\" \n","target_similarS = \"Helsinki-NLP/opus-mt-en-es\" \n","similarS_source = \"Helsinki-NLP/opus-mt-es-ca\"\n","similarS_target = \"Helsinki-NLP/opus-mt-es-en\"\n","target_similarT = \"Helsinki-NLP/opus-mt-en-de\"\n","similarT_source = \"Helsinki-NLP/opus-mt-de-ca\"\n","similarT_target = \"Helsinki-NLP/opus-mt-de-en\""],"metadata":{"id":"uZTH9jasPkG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define a translate function."],"metadata":{"id":"srXwPMhFzT0-"}},{"cell_type":"code","source":["def translate(model_name,input_file,output_file_source,output_file_target,num_lines=2000):\n","  # This function takes as input the name of the model it will translate with, the input file we want translated,\n","  # the output file with the truncated source language, the output file with the translated target language\n","  #and the number of lines of the file we want to translate.\n","\n","  # We initialize the model and the tokenizer\n","  model = MarianMTModel.from_pretrained(model_name)\n","  tokenizer = MarianTokenizer.from_pretrained(model_name)\n","\n","  # We open the input_file and store the lines in a variable\n","  f = open(input_file, \"r\")\n","  ff=f.readlines()\n","\n","  # We iterate over the lines with a range, that way we can select how many lines we want.\n","  for i in range(num_lines):\n","    # We open a file to store the translated lines\n","    tgt_file = open(output_file_target , \"a\") \n","\n","    # We select the line we want to translate\n","    src_text = ff[i]\n","    # We translate and detokenize the sentences\n","    translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n","    tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n","\n","    # We store the translated lines in a file\n","    tgt_file.write(\"\\n\")\n","    for text in tgt_text:\n","      tgt_file.write(text)\n","    tgt_file.close()\n","  \n","  # We store the number of lines of the source file in a file\n","  src_text = ff[:num_lines]\n","  with open(output_file_source , \"w\") as src_file:\n","    src_file.write(\"\\n\")\n","    for text in src_text:\n","      src_file.write(text)\n","  tgt_file.close()"],"metadata":{"id":"yRgFh36ESBUd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we use the *translate* function and the bash command *cat* to translate data and combine partial corpora to get our final corpora."],"metadata":{"id":"yopOmwAUzYYd"}},{"cell_type":"markdown","source":["Backtranslation from english to catalan.\n","\n"],"metadata":{"id":"xPdA0WUHtVk0"}},{"cell_type":"code","source":["translate(target_source,data_path+\"/mono.en\", data_path+\"/bckr.source.en\",data_path+\"/bckr.target.ca\")\n","!cat $data_path/train.ca $data_path/bckr.target.ca  > $data_path/bckr.train.ca\n","!cat $data_path/train.en $data_path/bckr.source.en  > $data_path/bckr.train.en"],"metadata":{"id":"tRGCOQ5xVs8N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Backtranslation from english to spanish."],"metadata":{"id":"UETnepCNM1PB"}},{"cell_type":"code","source":["translate(target_similarS,data_path+\"/mono.en\", data_path+\"/bckr.source.en\",data_path+\"/bckr.target.es\")\n","!cat $data_path/train.ca $data_path/bckr.target.es  > $data_path/bckr.train.es"],"metadata":{"id":"UmhddYOnMvJd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Backtranslation using a pivot language similar to source, from english to spanish to catalan."],"metadata":{"id":"lV_5bZ-WfYpH"}},{"cell_type":"code","source":["translate(target_similarS,data_path+\"/mono.en\", data_path+\"/bckr.source.en\",data_path+\"/bckr.target.es\")\n","translate(similarS_source,data_path+\"/bckr.target.es\", data_path+\"/bckr.source.es\",data_path+\"/bckr.target.ca.from_es\")\n","!cat $data_path/train.ca $data_path/bckr.target.ca.from_es  > $data_path/bckr.train.ca.from_es"],"metadata":{"id":"m7sfKgLTgaOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Backtranslation using a pivot language similar to source, from english to spanish to catalan, and also translating target, from english to spanish to english."],"metadata":{"id":"jerouibWnK46"}},{"cell_type":"code","source":["translate(target_similarS,data_path+\"/mono.en\", data_path+\"/bckr.source.en\",data_path+\"/bckr.target.es\")\n","translate(similarS_source,data_path+\"/bckr.target.es\", data_path+\"/bckr.source.es\",data_path+\"/bckr.target.ca.from_es\")\n","translate(similarS_target,data_path+\"/bckr.target.es\", data_path+\"/bckr.source.es\",data_path+\"/bckr.target.en.from_es\")\n","!cat $data_path/train.ca $data_path/bckr.target.ca.from_es  > $data_path/bckr.train.ca.from_es\n","!cat $data_path/train.en $data_path/bckr.target.en.from_es  > $data_path/bckr.train.en.from_es"],"metadata":{"id":"I_s1POuxnJJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Backtranslation using a pivot language similar to target, from english to german to catalan"],"metadata":{"id":"eR33ftv5TMil"}},{"cell_type":"code","source":["translate(target_similarT,data_path+\"/mono.en\", data_path+\"/bckr.source.en\",data_path+\"/bckr.target.de\")\n","translate(similarT_source,data_path+\"/bckr.target.de\", data_path+\"/bckr.source.de\",data_path+\"/bckr.target.ca.from_de\")\n","!cat $data_path/train.ca $data_path/bckr.target.ca.from_de  > $data_path/bckr.train.ca.from_de"],"metadata":{"id":"04Jvz_LxTCC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Backtranslation using a pivot language similar to target, from english to german to catalan, and translating target, from english to german to english"],"metadata":{"id":"eECmNEdr00Aw"}},{"cell_type":"code","source":["translate(target_similarT,data_path+\"/mono.en\", data_path+\"/bckr.source.en\",data_path+\"/bckr.target.de\")\n","translate(similarT_source,data_path+\"/bckr.target.de\", data_path+\"/bckr.source.de\",data_path+\"/bckr.target.ca.from_de\")\n","translate(similarT_target,data_path+\"/bckr.target.de\", data_path+\"/bckr.source.de\",data_path+\"/bckr.target.en.from_de\")\n","!cat $data_path/train.ca $data_path/bckr.target.ca.from_de  > $data_path/bckr.train.ca.from_de\n","!cat $data_path/train.en $data_path/bckr.target.en.from_de  > $data_path/bckr.train.en.from_de"],"metadata":{"id":"x_1DpL07zYN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Copying"],"metadata":{"id":"7dK3W11dUEnc"}},{"cell_type":"markdown","source":["We copied the the instances of the target language in the source part and we put it again duplicated in the target."],"metadata":{"id":"JTCrj2hPUJhs"}},{"cell_type":"code","source":["!cat  $data_path/train.ca $data_path/train.en > $data_path/train.copy.ca\n","!cat  $data_path/train.en $data_path/train.en > $data_path/train.copy.en"],"metadata":{"id":"HtnnWdAFUXx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Synonyms\n","For this technique we will use WordNet, available in the NLTK package, to substitute some words for their synonyms."],"metadata":{"id":"s-5osHALMGyb"}},{"cell_type":"markdown","source":["We define three functions: one to output a synonym given a word, another one to substitute some words of a sentence for their synonyms and the last one to create a file out of an input file and substitue words for their synonyms on each line."],"metadata":{"id":"5Aphozpy1Ga9"}},{"cell_type":"code","source":["def get_synonym(w):\n","  # We input a word and get a synonym that can be a word or a multiword entity\n","  synonyms = []\n","\n","  #We iterate over the wordnet synsets of a word\n","  for syn in wn.synsets(w):\n","    #We get all words for that synset and we append it to the list of synonyms\n","    for i in syn.lemmas():\n","        synonyms.append(i.name())\n","\n","  #If the synonym list is not empty we pick one word of the list randomly/ if not we return the same word\n","  if synonyms:\n","    n = random.randint(0,len(synonyms)-1)\n","    return synonyms[n].replace(\"_\",\" \")\n","  else:\n","    return w\n","\n","def replace_synonym_sentence(s):\n","  # We input a sentence and get the same with synonym substitution\n","\n","  # We do a list with the sentence\n","  sentence_list = s.split(\" \")\n","  c= 0\n","\n","  # We iterate the sentence list\n","  for word in sentence_list:\n","    num = random.randint(1,100)\n","    # We substitute each word by a synonym with a probability of 70%\n","    if num < 70:\n","      if word.endswith(\".\") or word.endswith(\",\") or word.endswith(\"?\") or word.endswith(\"!\"):\n","        word_wp = word[:-1]\n","        sentence_list[c] = get_synonym(word_wp)+word[-1]\n","      else:\n","        sentence_list[c] = get_synonym(word)\n","    c+=1\n","  return \" \".join(sentence_list)\n","\n","def replace_synonym_file(input_file, output_file):\n","  # We input 2 file paths the first one the one you want to subtitute and the other one the place you want to save it\n","  f = open(input_file, \"r\")\n","\n","  # We read the input file\n","  ff=f.readlines()\n","  with open(output_file , \"w\") as tgt_file:\n","    tgt_file.write(\"\\n\")\n","    for text in ff:\n","      # For each line we will apply the replace_synonym_sentence method\n","      tgt_file.write(replace_synonym_sentence(text))\n","  tgt_file.close()"],"metadata":{"id":"r91rtVMNMLdR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use the last function and the *cat* command to create our enlarged dataset by substituting words in English for their synonyms.  "],"metadata":{"id":"4I4YoE50yHzC"}},{"cell_type":"code","source":["replace_synonym_file(data_path+\"/train.en\",data_path+\"/syn.target.en\")\n","!cat $data_path/train.en $data_path/syn.target.en  > $data_path/syn.train.en\n","!cat $data_path/train.ca $data_path/train.ca > $data_path/syn.train.ca"],"metadata":{"id":"J00cHAeuMKhv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Combining\n","We combine all the partial corpora."],"metadata":{"id":"YzFHcf9dUEaK"}},{"cell_type":"code","source":["!cat $data_path/train.ca $data_path/bckr.target.ca $data_path/bckr.target.es $data_path/bckr.target.ca.from_es $data_path/bckr.target.ca.from_es $data_path/bckr.target.ca.from_de $data_path/bckr.target.ca.from_de $data_path/train.en > $data_path/train.monster.ca\n","!cat $data_path/train.en $data_path/bckr.source.en  $data_path/bckr.source.en $data_path/bckr.source.en $data_path/bckr.target.en.from_es $data_path/bckr.source.en $data_path/bckr.target.en.from_de $data_path/train.en > $data_path/train.monster.en"],"metadata":{"id":"Vb6PvV1QUqRE"},"execution_count":null,"outputs":[]}]}